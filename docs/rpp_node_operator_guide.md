# `rpp-node` Operator Guide

This guide documents the CLI tooling that ships with the repository. The
`rpp-node` binary hosts the runtime launchers for every supported mode and the
validator maintenance subcommands. No standalone `rpc-cli` tool exists in this
workspaceâ€”the shipped operator interface is the `rpp-node` CLI and the REST/RPC
workflows exposed by the running node.

> **PhaseÂ 2 update:** The `backend-plonky3` feature now enables the vendor
> Plonky3 prover/verifier pipeline. Production builds may target either the
> STWO (`prover-stwo`/`prover-stwo-simd`) or Plonky3 backend; only the
> deterministic mock backend remains blocked in release artefacts. Use the
> release pipeline checklist to verify that binaries are compiled with one of
> the production backends and that mock features are absent from build metadata.
> [`feature_guard.rs`](../rpp/node/src/feature_guard.rs) Â·
> [`build_release.sh`](../scripts/build_release.sh) Â·
> [`verify_release_features.sh`](../scripts/verify_release_features.sh) Â·
> [`ensure_prover_backend`](../rpp/node/src/lib.rs) Â·
> [Release pipeline checklist](../RELEASE.md#release-pipeline-checklist)
> Â· [Plonky3 runbook](./runbooks/plonky3.md)

## Build and install the CLI

Compile the binary with the release profile and select the backend that matches
the deployment tier. The build installs the multiplexer binary at
`target/release/rpp-node` and enables validator functionality required in
staging and production deployments.ã€F:docs/validator_quickstart.mdâ€ L24-L56ã€‘

```sh
# STWO backend
cargo build --release -p rpp-node --no-default-features --features prod,prover-stwo

# Plonky3 backend
cargo build --release -p rpp-node --no-default-features --features prod,backend-plonky3
```

The automated release pipeline exports `RPP_RELEASE_BASE_FEATURES` before
invoking `scripts/build_release.sh`. Point the variable to
`"prod,prover-stwo"`, `"prod,prover-stwo-simd"`, or `"prod,backend-plonky3"`
depending on the backend you intend to ship; the script always forces
`--no-default-features --features "$RPP_RELEASE_BASE_FEATURES"` so every
published artifact includes a production prover and the mock backend remains
disabled.ã€F:.github/workflows/release.ymlâ€ L115-L158ã€‘ã€F:scripts/build_release.shâ€ L1-L118ã€‘
Local builds should mirror the same flag set shown in
`scripts/build_release.sh` to avoid shipping binaries that fail at runtime due
to a missing production backend.

The helper `scripts/verify_release_features.sh` checks the compiled metadata and
fails when the mock prover slips into the feature list; run it as part of
pre-release validation. The compile-time guard mirrors this behaviour by
preventing `backend-plonky3` and `prover-mock` from being enabled at the same
time.ã€F:scripts/verify_release_features.shâ€ L1-L146ã€‘ã€F:rpp/node/src/feature_guard.rsâ€ L1-L5ã€‘

Keep the repository cloned on the host to rebuild quickly when upgrades ship.

## Runtime launchers

The `rpp-node` binary dispatches to four runtime modes and accepts the shared
runtime options (configuration paths, telemetry overrides, dry runs, log
formatting, and networking flags).ã€F:rpp/node/src/main.rsâ€ L28-L75ã€‘ã€F:rpp/node/src/lib.rsâ€ L35-L314ã€‘

```text
rpp-node node [runtime options]
rpp-node wallet [runtime options]
rpp-node hybrid [runtime options]
rpp-node validator [runtime options] [validator subcommand]
```

Pass `--config`/`--wallet-config` to target custom configuration files or rely
on the precedence chain described in the validator quickstart when the defaults
are sufficient.ã€F:docs/validator_quickstart.mdâ€ L62-L111ã€‘ Add `--dry-run` to
validate configuration without starting long-running tasks; the CLI exits after
bootstrap so operators can gate deployments in CI.ã€F:docs/validator_quickstart.mdâ€ L195-L210ã€‘

### Plonky3 backend (Phase 2)

`backend-plonky3` aktiviert jetzt den produktiven Plonky3-Prover und -Verifier.
Die Wallet- und Node-Adapter erzeugen und prÃ¼fen Vendor-Beweise Ã¼ber dieselben
Traits wie das STWO-Backend, sodass Keygen-, Prover- und Verifier-Hooks im
Produktionspfad identisch orchestriert werden.ã€F:rpp/proofs/plonky3/prover/mod.rsâ€ L19-L520ã€‘ã€F:rpp/proofs/plonky3/verifier/mod.rsâ€ L1-L212ã€‘

Das [Plonky3-Runbook](./runbooks/plonky3.md) beschreibt den vollstÃ¤ndigen
Operator-Workflow:

- Circuit-Caches vorbereiten (`rpp-node` legt Proving-/Verifying-Keys im
  Artefaktverzeichnis ab und spiegelt den Zustand Ã¼ber
  `backend_health.plonky3.*`).
- Proof-Generierung und -Verifikation Ã¼berwachen (`rpp.runtime.proof.generation.*`
  und `rpp.runtime.proof.verification.*` mit Labels `backend="plonky3"`,
  `proof="transaction|state|pruning|consensus"`).
- Acceptance-Kriterien prÃ¼fen: die Phaseâ€‘2-Grenzwerte orientieren sich an den
  Messwerten aus `tools/simnet/scenarios/consensus_quorum_stress.ron` und sind in
  `performance/consensus_proofs.md` dokumentiert.

Das Nightly-Szenario `consensus-quorum-stress` treibt den Prover mit hoher
Validator- und Witness-Last sowie absichtlich manipulierten VRF-/Quorum-Daten
an. `scripts/analyze_simnet.py` wertet die JSON-Summary aus, bricht bei
Ãœberschreitung der p95-Grenzen ab und meldet unerwartete Tamper-Erfolge. Die
GegenÃ¼berstellung von Erfolgs- und Fehlerpfaden ist ebenfalls im Runbook
festgehalten.ã€F:tools/simnet/scenarios/consensus_quorum_stress.ronâ€ L1-L22ã€‘ã€F:scripts/analyze_simnet.pyâ€ L1-L200ã€‘ã€F:docs/performance/consensus_proofs.mdâ€ L1-L160ã€‘

Die Konsensus-Beweise werden zusÃ¤tzlich softwareseitig gehÃ¤rtet: `ConsensusCircuit`
im Backend bindet VRF-Ausgaben, -Beweise und Quorum-Digests an den Block-Hash,
die Wallet-Adapter lassen nur valide Zeugen in die Prover-Pipeline, und der
Verifier rekonstruiert die Bindings vor der Proof-PrÃ¼fung. Regressionstests in
`tests/consensus/plonky3_consensus.rs` sowie im Backend sichern diese Checks gegen
Regressionen.ã€F:prover/plonky3_backend/src/circuits/consensus.rsâ€ L1-L245ã€‘ã€F:rpp/proofs/plonky3/prover/mod.rsâ€ L123-L520ã€‘ã€F:rpp/proofs/plonky3/verifier/mod.rsâ€ L1-L212ã€‘ã€F:tests/consensus/plonky3_consensus.rsâ€ L1-L134ã€‘

Grafana-Panels unter `docs/dashboards/consensus_proof_validation.json`
visualisieren diese Kennzahlen (Latenzen, Fehlerraten, Circuit-Cache-GrÃ¶ÃŸe) fÃ¼r
Plonky3 und werden vom CI-Dashboard-Lint Ã¼berprÃ¼ft. Binde die Panels in das
Produktions-Dashboard ein, um Phaseâ€‘2-Abnahmekriterien sichtbar zu machen.ã€F:docs/dashboards/consensus_proof_validation.jsonâ€ L1-L120ã€‘

Kombiniere `backend-plonky3` nicht mit dem `prover-mock`-Feature; der Guard
erzwingt weiterhin die Trennung zwischen deterministischen Test-Fixtures und
produktiven Vendor-Artefakten.ã€F:rpp/node/src/feature_guard.rsâ€ L1-L5ã€‘

## Validator tooling

Invoke validator-specific helpers through `rpp-node validator`. Subcommands cover
VRF rotation, telemetry diagnostics, and uptime proof management, all backed by
the active node configuration.ã€F:rpp/node/src/main.rsâ€ L48-L183ã€‘ Detailed
workflowsâ€”including sample invocations and expected outputâ€”live in the
[validator tooling guide](./validator_tooling.md).ã€F:docs/validator_tooling.mdâ€ L14-L137ã€‘

### Consensus proof metadata expectations

Finality proofs now encode the epoch/slot context, VRF proofs, and quorum
evidence roots inside the public inputs. Operators must ensure that
`consensus.metadata` in block production includes:

- `epoch`/`slot` counters that match the fork-choice state machine.
- Hex-encoded `quorum_bitmap_root` and `quorum_signature_root` digests from the
  aggregated vote sets.
- VollstÃ¤ndige `vrf_entries` inklusive Randomness, Pre-Output, Proof, Public
  Key sowie Poseidon-Metadaten (`Digest`, `Last Block Header`, `Epoch`,
  `Tier Seed`) fÃ¼r jede Validator:in im Zertifikat. Ã„ltere Clients kÃ¶nnen die bisherigen
  `vrf_outputs`/`vrf_proofs` aus diesen EintrÃ¤gen ableiten, solange sie das
  Version-Flag `version=2` setzen.

Setze bei RPC-Checks nach MÃ¶glichkeit `version=3`, um die strukturierten
EintrÃ¤ge inklusive Public Keys und Poseidon-Digests zu erhalten. TemporÃ¤re
KompatibilitÃ¤ts-Pipelines dÃ¼rfen weiterhin `version=2` anfordern, sollten aber
das Downstream-Mapping aus `vrf_entries` dokumentieren, um den Wechsel
nachvollziehbar zu halten.

Missing or inconsistent values cause the verifier to reject the consensus proof
bundle, so double-check the witness payload when diagnosing failed block
imports.ã€F:docs/consensus/finality_proof_story.mdâ€ L1-L38ã€‘

Release-Builds listen Circuit-Versionen, Constraint-ZÃ¤hlungen und unterstÃ¼tzte Backends
in den [Release-Notizen](docs/release_notes.md); ziehe die Tabelle bei Audits oder
Rollback-PlÃ¤nen hinzu, um sicherzustellen, dass Operator:innen identische Proof-Artefakte
ausrollen.ã€F:docs/release_notes.mdâ€ L1-L160ã€‘

### Phaseâ€¯2 consensus proof validation checks

Phaseâ€¯2 verlangt nachvollziehbare Belege, dass manipulierte VRF-/Quorum-Daten an
der Validator-Schnittstelle scheitern. Nutze die folgenden Schritte, um beide
Pfadvarianten abzudecken:

1. **Lokale Reproduktions-Tests ausfÃ¼hren.** `cargo xtask test-consensus-manipulation`
   baut einen gÃ¼ltigen Konsens-Proof, permutiert anschlieÃŸend VRF-Outputs sowie
   die Quorum-Roots und erwartet Fehler vom Verifier. Aktiviere die gewÃ¼nschten
   Backends mit `XTASK_FEATURES`:

   ```sh
   # Plonky3
   cargo xtask test-consensus-manipulation \
     --features backend-plonky3 --no-default-features

   # STWO
   XTASK_NO_DEFAULT_FEATURES=1 XTASK_FEATURES="prod,prover-stwo" \
     cargo xtask test-consensus-manipulation
   ```

   Jeder Lauf muss mit Exit-Code `0` enden. Ein Fehlschlag signalisiert, dass
   Manipulationen akzeptiert wurden oder der Proof nicht mehr erzeugt werden
   konnte.ã€F:xtask/src/main.rsâ€ L1-L120ã€‘ã€F:tests/consensus/consensus_certificate_tampering.rsâ€ L1-L160ã€‘

2. **Simnet-Szenario fÃ¼r Tamper-Versuche fahren.** Starte das Stresstest-Szenario,
   das valide und manipulierte BlÃ¶cke gegeneinander laufen lÃ¤sst:

   ```sh
   cargo run -p simnet -- \
     --scenario tools/simnet/scenarios/consensus_quorum_stress.ron \
     --artifacts-dir target/simnet/consensus-quorum
   ```

   Die erzeugten Logs (`target/simnet/consensus-quorum/node.log`) enthalten
   positive Pfade (`consensus witness participants do not match commit set` fÃ¼r
   Manipulationen, `local consensus proof rejected by verifier` bei invaliden
   Zeugen). Erfolgreiche BlÃ¶cke tauchen weiterhin im Telemetrie-Stream auf und
   erhÃ¶hen `consensus_quorum_verifications_total{result="success"}`.ã€F:tools/simnet/scenarios/consensus_quorum_stress.ronâ€ L1-L22ã€‘ã€F:rpp/runtime/node.rsâ€ L6314-L6393ã€‘ã€F:rpp/runtime/types/block.rsâ€ L2002-L2245ã€‘

3. **RPC-Validierung dokumentieren.** WÃ¤hrend eines Testlaufs liest
   `GET /status/consensus` den aktuellen Quorum-Status; bei erfolgreichen BlÃ¶cken
   ist `quorum_reached=true` und `round` steigt monoton. FÃ¼hren Tamper-Versuche
   zu Ablehnungen, bleibt `quorum_reached=false` und die Log-Datei enthÃ¤lt den
   Fehlergrund (`invalid VRF proof`, `duplicate precommit detected`, â€¦). Dokumentiere
   beide Antworten in deinem Abnahmeprotokoll.ã€F:rpp/rpc/api.rsâ€ L1374-L2015ã€‘ã€F:rpp/runtime/node.rsâ€ L358-L390ã€‘

4. **Metrik-Snapshots exportieren.** Phaseâ€‘2 verlangt Nachweise Ã¼ber die neuen
   Histogramme/Counters. Verwende deinen Prometheus-/OTLP-Endpunkt oder die
   bereitgestellten Grafana-Panels (`docs/dashboards/consensus_grafana.json`), um
   `consensus_vrf_verification_time_ms{result="success"}` und
   `consensus_quorum_verifications_total{result="failure"}` zu visualisieren. Die
   Aufzeichnungen gehÃ¶ren in das Abnahme-Log deiner Betriebsabteilung.ã€F:rpp/runtime/telemetry/metrics.rsâ€ L60-L339ã€‘ã€F:docs/dashboards/consensus_grafana.jsonâ€ L1-L200ã€‘

> ğŸ’¡ ErgÃ¤nze jeden Testlauf in der [Observability-Checkliste](./runbooks/observability.md#phase-2-consensus-proof-audits)
> und verlinke die Log-/Dashboard-Screenshots, damit Auditor:innen die Belege
> schnell nachvollziehen kÃ¶nnen.

Common tasks include:

```sh
# Rotate VRF keys using the configured secrets backend
rpp-node validator vrf rotate --config config/validator.toml

# Inspect collector health by querying the validator telemetry endpoint
rpp-node validator telemetry --rpc-url http://127.0.0.1:7070 --auth-token $RPP_RPC_TOKEN --pretty

# Submit and inspect uptime proofs via the validator RPC
rpp-node validator uptime submit --wallet-config config/wallet.toml --auth-token $RPP_RPC_TOKEN
rpp-node validator uptime status --rpc-url http://127.0.0.1:7070 --auth-token $RPP_RPC_TOKEN --json
```

State-sync and head monitoring rely on the public `/state-sync` RPC endpoints;
use `curl`/`wget` or similar tooling to consume the SSE stream and fetch
snapshot chunks as outlined in the validator tooling guide.ã€F:docs/validator_tooling.mdâ€ L53-L118ã€‘

## RPC authentication & rate limiting

Node RPC endpoints support optional bearer-token authentication and per-client
rate limiting. Supply the configured token with the `--auth-token` flag when
using CLI helpers or add an `Authorization: Bearer <token>` header to curl
requests.ã€F:docs/API_SECURITY.mdâ€ L10-L37ã€‘ã€F:rpp/node/src/main.rsâ€ L101-L178ã€‘ The
flag must match the token configured for the active RPC endpoint; omit it only
when authentication is disabled. Secure configurations should rotate tokens
alongside other secrets and audit usage via reverse-proxy logs.

Expose the RPC to browser dashboards by setting `network.rpc.allowed_origin` in
configuration. Use `--rpc-allowed-origin` for one-off overrides (pass an empty
string to clear the allow-list) and restart without the flag to fall back to the
profile defaults.ã€F:docs/API_SECURITY.mdâ€ L38-L58ã€‘

When automation calls the REST endpoints directly, reuse the same tokens and
respect the configured request limits. A `429` response indicates that the node's
rate limiter rejected the request; retry with exponential backoff or throttle
callers as described in the [deployment & observability playbook](./deployment_observability.md).ã€F:docs/deployment_observability.mdâ€ L1-L61ã€‘

## Example RPC workflows

Automation typically interacts with the node via authenticated HTTP requests.
The pruning runbook outlines the snapshot APIs, including example `curl`
invocations that enqueue pruning work and inspect receipts.ã€F:docs/runbooks/pruning.mdâ€ L1-L102ã€‘
Additional operational runbooks cover startup validation, telemetry wiring, and
upgrade procedures when rolling new binaries into service.ã€F:docs/runbooks/startup.mdâ€ L1-L40ã€‘ã€F:docs/runbooks/observability.mdâ€ L1-L120ã€‘ã€F:docs/runbooks/upgrade.mdâ€ L1-L60ã€‘

Pair this guide with the validator quickstart and troubleshooting references to
fully provision and maintain a production node.ã€F:docs/validator_quickstart.mdâ€ L1-L238ã€‘ã€F:docs/validator_troubleshooting.mdâ€ L1-L140ã€‘
