# `rpp-node` Operator Guide

This guide documents the CLI tooling that ships with the repository. The
`rpp-node` binary hosts the runtime launchers for every supported mode and the
validator maintenance subcommands. Operators should run `cargo run -p rpp-chain
-- â€¦` for configuration checks, dry-run validation, and validator toolingâ€”the
stub binary exercises the shared CLI surface without booting the runtime. The
`rpp-node` binary and its mode-specific entry points remain reserved for
production deployments and on-host supervisors. No standalone `rpc-cli` tool
exists in this workspaceâ€”the shipped operator interface is the unified CLI plus
the REST/RPC workflows exposed by the running node.

> **PhaseÂ 2 update:** The `backend-plonky3` feature now enables the vendor
> Plonky3 prover/verifier pipeline. Production builds may target either the
> STWO (`prover-stwo`/`prover-stwo-simd`) or Plonky3 backend; only the
> deterministic mock backend remains blocked in release artefacts. Use the
> release pipeline checklist to verify that binaries are compiled with one of
> the production backends and that mock features are absent from build metadata.
> [`feature_guard.rs`](../rpp/node/src/feature_guard.rs) Â·
> [`build_release.sh`](../scripts/build_release.sh) Â·
> [`verify_release_features.sh`](../scripts/verify_release_features.sh) Â·
> [`ensure_prover_backend`](../rpp/node/src/lib.rs) Â·
> [Release pipeline checklist](../RELEASE.md#release-pipeline-checklist) Â·
> [Phaseâ€‘2 Acceptance Checklist](./runbooks/phase2_acceptance.md)
> Â· [Plonky3 runbook](./runbooks/plonky3.md)
> Â· [Incident Runbook: rpp-stark verification failures](./zk_backends.md#incident-runbook-rpp-stark-verification-failures)

## Build and install the CLI

Compile the binary with the release profile and select the backend that matches
the deployment tier. The build installs the multiplexer binary at
`target/release/rpp-node` and enables validator functionality required in
staging and production deployments.ã€F:docs/validator_quickstart.mdâ€ L24-L56ã€‘

```sh
# STWO backend
cargo build --release -p rpp-node --no-default-features --features prod,prover-stwo

# Plonky3 backend
cargo build --release -p rpp-node --no-default-features --features prod,backend-plonky3
```

The automated release pipeline exports `RPP_RELEASE_BASE_FEATURES` before
invoking `scripts/build_release.sh`. Point the variable to
`"prod,prover-stwo"`, `"prod,prover-stwo-simd"`, or `"prod,backend-plonky3"`
depending on the backend you intend to ship; the script always forces
`--no-default-features --features "$RPP_RELEASE_BASE_FEATURES"` so every
published artifact includes a production prover and the mock backend remains
disabled.ã€F:.github/workflows/release.ymlâ€ L115-L158ã€‘ã€F:scripts/build_release.shâ€ L1-L118ã€‘
Local builds should mirror the same flag set shown in
`scripts/build_release.sh` to avoid shipping binaries that fail at runtime due
to a missing production backend.

The helper `scripts/verify_release_features.sh` checks the compiled metadata and
fails when the mock prover slips into the feature list; run it as part of
pre-release validation. The compile-time guard mirrors this behaviour by
preventing `backend-plonky3` and `prover-mock` from being enabled at the same
time.ã€F:scripts/verify_release_features.shâ€ L1-L146ã€‘ã€F:rpp/node/src/feature_guard.rsâ€ L1-L5ã€‘

Keep the repository cloned on the host to rebuild quickly when upgrades ship.

## Runtime launchers

The `rpp-node` binary dispatches to four runtime modes and accepts the shared
runtime options (configuration paths, telemetry overrides, dry runs, log
formatting, and networking flags).ã€F:rpp/node/src/main.rsâ€ L28-L75ã€‘ã€F:rpp/node/src/lib.rsâ€ L35-L314ã€‘

```text
rpp-node node [runtime options]
rpp-node wallet [runtime options]
rpp-node hybrid [runtime options]
rpp-node validator [runtime options] [validator subcommand]
```

Pass `--config`/`--wallet-config` to target custom configuration files or rely
on the precedence chain described in the validator quickstart when the defaults
are sufficient.ã€F:docs/validator_quickstart.mdâ€ L62-L111ã€‘ Use `cargo run -p
rpp-chain -- <mode> --dry-run --config <path>` to validate configuration without
starting long-running tasks; the CLI exits after bootstrap so operators can gate
deployments in CI while the production binary stays reserved for supervisors and
release artefacts.ã€F:docs/validator_quickstart.mdâ€ L195-L210ã€‘

> **Networking reminder:** Whenever a runtime mode changes the node profile,
> re-apply the [gossip tuning checklist](./networking.md#gossip-tuning-checklist)
> to confirm that gossip bandwidth, allowlists, and replay windows align with
> the new configuration before promoting the change to staging or production.

### Plonky3 backend (Phase 2)

`backend-plonky3` aktiviert jetzt den produktiven Plonky3-Prover und -Verifier.
Die Wallet- und Node-Adapter erzeugen und prÃ¼fen Vendor-Beweise Ã¼ber dieselben
Traits wie das STWO-Backend, sodass Keygen-, Prover- und Verifier-Hooks im
Produktionspfad identisch orchestriert werden.ã€F:rpp/proofs/plonky3/prover/mod.rsâ€ L19-L520ã€‘ã€F:rpp/proofs/plonky3/verifier/mod.rsâ€ L1-L212ã€‘

Das [Plonky3-Runbook](./runbooks/plonky3.md) beschreibt den vollstÃ¤ndigen
Operator-Workflow:

- Circuit-Caches vorbereiten (`rpp-node` legt Proving-/Verifying-Keys im
  Artefaktverzeichnis ab und spiegelt den Zustand Ã¼ber
  `backend_health.plonky3.*`).
- Proof-Generierung und -Verifikation Ã¼berwachen (`rpp.runtime.proof.generation.duration`
  / `.size` / `.count` mit `backend="plonky3"`, `proof_kind="transaction|state|pruning|consensus"`
  sowie `rpp_stark_verify_duration_seconds` und die zugehÃ¶rigen Byte-Histogramme
  mit `proof_kind="*"`).ã€F:rpp/runtime/telemetry/metrics.rsâ€ L426-L520ã€‘
- Acceptance-Kriterien prÃ¼fen: die Phaseâ€‘2-Grenzwerte orientieren sich an den
  Messwerten aus `tools/simnet/scenarios/consensus_quorum_stress.ron` und sind in
  `performance/consensus_proofs.md` dokumentiert.

Das Nightly-Szenario `consensus-quorum-stress` treibt den Prover mit hoher
Validator- und Witness-Last sowie absichtlich manipulierten VRF-/Quorum-Daten
an. `scripts/analyze_simnet.py` wertet die JSON-Summary aus, bricht bei
Ãœberschreitung der p95-Grenzen ab und meldet unerwartete Tamper-Erfolge. Die
GegenÃ¼berstellung von Erfolgs- und Fehlerpfaden ist ebenfalls im Runbook
festgehalten.ã€F:tools/simnet/scenarios/consensus_quorum_stress.ronâ€ L1-L22ã€‘ã€F:scripts/analyze_simnet.pyâ€ L1-L200ã€‘ã€F:docs/performance/consensus_proofs.mdâ€ L1-L160ã€‘

Die Konsensus-Beweise werden zusÃ¤tzlich softwareseitig gehÃ¤rtet: `ConsensusCircuit`
im Backend bindet VRF-Ausgaben, -Beweise und Quorum-Digests an den Block-Hash,
die Wallet-Adapter lassen nur valide Zeugen in die Prover-Pipeline, und der
Verifier rekonstruiert die Bindings vor der Proof-PrÃ¼fung. Regressionstests in
`tests/consensus/plonky3_consensus.rs` sowie im Backend sichern diese Checks gegen
Regressionen.ã€F:prover/plonky3_backend/src/circuits/consensus.rsâ€ L1-L245ã€‘ã€F:rpp/proofs/plonky3/prover/mod.rsâ€ L123-L520ã€‘ã€F:rpp/proofs/plonky3/verifier/mod.rsâ€ L1-L212ã€‘ã€F:tests/consensus/plonky3_consensus.rsâ€ L1-L134ã€‘

Grafana-Panels unter `docs/dashboards/consensus_proof_validation.json`
visualisieren diese Kennzahlen (Latenzen, Fehlerraten, Circuit-Cache-GrÃ¶ÃŸe) fÃ¼r
Plonky3 und werden vom CI-Dashboard-Lint Ã¼berprÃ¼ft. Binde die Panels in das
Produktions-Dashboard ein, um Phaseâ€‘2-Abnahmekriterien sichtbar zu machen.ã€F:docs/dashboards/consensus_proof_validation.jsonâ€ L1-L120ã€‘

Kombiniere `backend-plonky3` nicht mit dem `prover-mock`-Feature; der Guard
erzwingt weiterhin die Trennung zwischen deterministischen Test-Fixtures und
produktiven Vendor-Artefakten.ã€F:rpp/node/src/feature_guard.rsâ€ L1-L5ã€‘

## Validator tooling

Invoke validator-specific helpers through `cargo run -p rpp-chain -- validator`.
Subcommands cover
VRF rotation, telemetry diagnostics, and uptime proof management, all backed by
the active node configuration.ã€F:rpp/node/src/main.rsâ€ L48-L183ã€‘ Detailed
workflowsâ€”including sample invocations and expected outputâ€”live in the
[validator tooling guide](./validator_tooling.md).ã€F:docs/validator_tooling.mdâ€ L14-L137ã€‘

### Snapshot streaming CLI

`cargo run -p rpp-chain -- validator snapshot` wraps the `/p2p/snapshots` RPCs so operators can
start, resume, inspect, and cancel consumer sessions without constructing HTTP
requests by hand. The CLI resolves the active validator configuration, derives
the RPC base URL from `network.rpc.listen`, and automatically attaches the
configured bearer token unless an explicit `--auth-token` override is provided.

```text
$ cargo run -p rpp-chain -- validator snapshot start --peer 12D3KooWexamplePeer
snapshot session started:
  session: 42
  peer: 12D3KooWexamplePeer
  root: deadbeefcafebabe
  last_chunk_index: none
  last_update_index: none
  last_update_height: none
  verified: unknown
  error: none

$ cargo run -p rpp-chain -- validator snapshot status --session 42
snapshot status:
  session: 42
  peer: 12D3KooWexamplePeer
  root: deadbeefcafebabe
  last_chunk_index: none
  last_update_index: none
  last_update_height: none
  verified: unknown
  error: none

$ cargo run -p rpp-chain -- validator snapshot resume --session 42 --peer 12D3KooWexamplePeer --plan-id plan-2024-05-18
snapshot session resumed:
  session: 42
  peer: 12D3KooWexamplePeer
  root: deadbeefcafebabe
  last_chunk_index: 12
  last_update_index: 3
  last_update_height: 256
  verified: false
  error: none

$ cargo run -p rpp-chain -- validator snapshot cancel --session 42
snapshot session 42 cancelled
```

Errors propagate directly from the RPC surface so operators receive the HTTP
status code and body when a request fails (for example: `RPC returned 500:
intentional failure`). The behaviour replaces the manual `curl` workflows, adds
token management, and prints structured output for incident logs and the
Phaseâ€‘3 artefaktablage.ã€F:rpp/node/src/main.rsâ€ L118-L310ã€‘ã€F:docs/runbooks/phase3_acceptance.mdâ€ L8-L62ã€‘ Dokumentiere jede
Snapshot-Intervention im [On-Call-Handbuch](./runbooks/oncall.md#snapshot-recovery)
und halte die Metriken parallel Ã¼ber das [Observability-Runbook](./runbooks/observability.md#snapshot-cli-diagnose) fest, damit
Audit- und Dashboard-Belege synchron bleiben.ã€F:docs/runbooks/oncall.mdâ€ L21-L56ã€‘ã€F:docs/runbooks/observability.mdâ€ L6-L170ã€‘ Die Panels aus
`pipeline_overview.json`, `pipeline_proof_validation.json` und `vrf_overview.json`
visualisieren dieselben Fortschritts- und Fehlerindikatoren, die die CLI als
Text ausgibt, und sind verpflichtende Artefakte fÃ¼r die Phaseâ€‘3-Abnahme.ã€F:docs/dashboards/pipeline_overview.jsonâ€ L200-L260ã€‘ã€F:docs/dashboards/pipeline_proof_validation.jsonâ€ L1-L60ã€‘ã€F:docs/dashboards/vrf_overview.jsonâ€ L1-L60ã€‘

### Snapshot verification CLI

`cargo run -p rpp-chain -- validator snapshot verify` kapselt den Offline-Verifier aus
`tools/snapshot-verify` und nutzt die Validator-Konfiguration, um Manifest,
Signatur, Chunk-Verzeichnis und VerifierschlÃ¼ssel automatisch aufzulÃ¶sen. Ohne
Overrides liest der Befehl `<snapshot_dir>/manifest/chunks.json`, erwartet die
Signatur nebenan als `chunks.json.sig`, prÃ¼ft `<snapshot_dir>/chunks` und leitet
den Ed25519-SchlÃ¼ssel aus `timetoke_snapshot_key_path` ab. Optional kannst du
`--manifest`, `--signature`, `--chunk-root`, `--output` und `--public-key`
verwenden, um einzelne Pfade bzw. einen alternativen Public Key zu setzen.ã€F:rpp/node/src/main.rsâ€ L140-L227ã€‘ FÃ¼r eine praktische
Ãœbung inklusive Smoke-Fixtures und Artefakt-Checks folge dem
[Phaseâ€‘A Operator Lab](training/phaseA_operator_lab.md).

> **Hinweis:** Die Runtime streamt keine Snapshots mehr, wenn die
> zugehÃ¶rige `.sig`-Datei fehlt oder kein gÃ¼ltiges Base64 enthÃ¤lt. Stelle bei
> jeder VerÃ¶ffentlichung sicher, dass Payload und Signatur gemeinsam rotiert
> werden (z.â€¯B. durch `rename(2)` auf ein vorbereitetes Verzeichnis), damit
> Konsumenten keine unsignierten Manifeste sehen.

```text
$ cargo run -p rpp-chain -- validator snapshot verify --config config/validator.toml
{
  "manifest_path": "./data/snapshots/manifest/chunks.json",
  "signature_path": "./data/snapshots/manifest/chunks.json.sig",
  "public_key_path": "./keys/timetoke_snapshot.toml",
  "chunk_root": "./data/snapshots/chunks",
  "signature": {
    "algorithm": "ed25519",
    "manifest_digest": "â€¦",
    "public_key_fingerprint": "â€¦",
    "signature_valid": true,
    "error": null
  },
  "summary": {
    "segments_total": 1,
    "verified": 1,
    "checksum_mismatches": 0,
    â€¦
  },
  "errors": []
}
```

Bei einer Abweichung liefert der Befehl weiterhin den JSON-Report, beendet sich
aber mit Exit-Code `3`, sodass CI-Jobs und Runbooks zwischen Signaturfehlern
(Exit-Code `2`) und tatsÃ¤chlichen Chunk-Abweichungen unterscheiden kÃ¶nnen.ã€F:rpp/node/src/main.rsâ€ L210-L227ã€‘ã€F:rpp/node/tests/snapshot_verify.rsâ€ L1-L123ã€‘ Dokumentiere die Reports als Teil der
Snapshot-Abnahme und bewahre sie gemeinsam mit den Release-Artefakten auf.

### Consensus proof metadata expectations

Finality proofs now encode the epoch/slot context, VRF proofs, and quorum
evidence roots inside the public inputs. Operators must ensure that
`consensus.metadata` in block production includes:

- `epoch`/`slot` counters that match the fork-choice state machine.
- Hex-encoded `quorum_bitmap_root` and `quorum_signature_root` digests from the
  aggregated vote sets.
- VollstÃ¤ndige `vrf_entries` inklusive Randomness, Pre-Output, Proof, Public
  Key sowie Poseidon-Metadaten (`Digest`, `Last Block Header`, `Epoch`,
  `Tier Seed`) fÃ¼r jede Validator:in im Zertifikat. `Last Block Header` muss dem
  Zertifikats-`block_hash` entsprechen und die `Epoch`-Zeichenkette dem
  exportierten `epoch`-ZÃ¤hler. Ã„ltere Clients kÃ¶nnen die bisherigen
  `vrf_outputs`/`vrf_proofs` aus diesen EintrÃ¤gen ableiten, solange sie das
  Version-Flag `version=2` setzen.

Setze bei RPC-Checks nach MÃ¶glichkeit `version=3`, um die strukturierten
EintrÃ¤ge inklusive Public Keys und Poseidon-Digests zu erhalten. TemporÃ¤re
KompatibilitÃ¤ts-Pipelines dÃ¼rfen weiterhin `version=2` anfordern, sollten aber
das Downstream-Mapping aus `vrf_entries` dokumentieren, um den Wechsel
nachvollziehbar zu halten.

Missing or inconsistent values cause the verifier to reject the consensus proof
bundle. The host now rejects VRF proof strings that do not expand to exactly
`crate::vrf::VRF_PROOF_LENGTH` bytes before the prover is invoked, so truncated
or padded transcripts surface as immediate metadata errors. Double-check the
witness payload when diagnosing failed block imports.ã€F:docs/consensus/finality_proof_story.mdâ€ L33-L44ã€‘

Release-Builds listen Circuit-Versionen, Constraint-ZÃ¤hlungen und unterstÃ¼tzte Backends
in den [Release-Notizen](release_notes.md); ziehe die Tabelle bei Audits oder
Rollback-PlÃ¤nen hinzu, um sicherzustellen, dass Operator:innen identische Proof-Artefakte
ausrollen.ã€F:docs/release_notes.mdâ€ L1-L160ã€‘

### Snapshot verifier workflow

Nutze zwei Pfade, um Snapshot-Bundles vor der Freigabe zu prÃ¼fen:

1. **CI/Smoke-Run:** `cargo xtask snapshot-verifier` erzeugt ein synthetisches
   Bundle unter `target/snapshot-verifier-smoke/`, signiert das Manifest und
   fÃ¼hrt `snapshot-verify` samt Aggregationsreport aus. Der Job `snapshot-verifier`
   im CI spiegelt denselben Ablauf und lÃ¤dt das Artefakt (`snapshot-verify-report.json` +
   `.sha256`) hoch, damit Reviewer:innen den Gate-Status nachvollziehen kÃ¶nnen.ã€F:xtask/src/main.rsâ€ L220-L318ã€‘ã€F:.github/workflows/ci.ymlâ€ L369-L397ã€‘
2. **Produktives Release:** Beim Verpacken laufen `scripts/build_release.sh` und der
   Release-Workflow `Build <target>` automatisch `snapshot-verify` fÃ¼r jedes reale
   Manifest. Stelle den VerifierschlÃ¼ssel via `SNAPSHOT_MANIFEST_PUBKEY_HEX` bereit und
   finde die Reports anschlieÃŸend unter `dist/artifacts/<target>/snapshot-verify-report.json`
   inklusive `.sha256`. Vergleiche den Hash (`sha256sum .../snapshot-verify-report.json`)
   mit dem Eintrag im Release-Notes-Abschnitt â€Snapshot verifier attestationâ€œ.ã€F:scripts/build_release.shâ€ L273-L348ã€‘ã€F:.github/workflows/release.ymlâ€ L150-L233ã€‘

Bewahre die Einzelreports (`*-verify.json`) gemeinsam mit dem Aggregat auf, damit
Auditor:innen das Ergebnis pro Manifest nachvollziehen kÃ¶nnen.

### WORM export validation

Der Audit-Log-Export lÃ¤sst sich lokal und in CI verifizieren:

- `cargo xtask test-worm-export` erzeugt unter `target/worm-export-smoke/`
  einen signierten Audit-Eintrag (`worm/*.json`), die Retention-Metadaten und die
  Summary `worm-export-summary.json`. Die Summary bestÃ¤tigt, dass jede Signatur
  mit dem erzeugten Key-Set Ã¼berprÃ¼ft wurde (`signature_valid=true`).ã€F:xtask/src/main.rsâ€ L120-L318ã€‘
- CI (`worm-export-smoke`) und Nightly (`worm-export`) verÃ¶ffentlichen das
  Artefakt `worm-export-smoke` mitsamt Summary, damit Auditor:innen Signaturen,
  Retention-Fenster und erzeugte JSON-Objekte prÃ¼fen kÃ¶nnen.ã€F:.github/workflows/ci.ymlâ€ L360-L387ã€‘ã€F:.github/workflows/nightly.ymlâ€ L10-L24ã€‘

Vor Produktionsexports: Stimme die endgÃ¼ltigen WORM-Endpoints mit Security &
Compliance ab und dokumentiere die Objekt-Storage-Konfiguration in den
Freigabeunterlagen.

### Phaseâ€¯2 consensus proof validation checks

Phaseâ€¯2 verlangt nachvollziehbare Belege, dass manipulierte VRF-/Quorum-Daten an
der Validator-Schnittstelle scheitern. Nutze zusÃ¤tzlich die
[Plonky3 Production Validation Checklist](./testing/plonky3_experimental_testplan.md#4-production-sign-off-checklist),
um die erforderlichen Artefakte fÃ¼r das Freigabeprotokoll abzuhaken.ã€F:docs/testing/plonky3_experimental_testplan.mdâ€ L1-L121ã€‘

#### Known-good vs. tampered replay drill

Nutze den Phaseâ€‘2-Workflow, um sowohl einen gÃ¼ltigen Block als auch abgelehnte
Manipulationen zu dokumentieren:

1. **Bekannten guten Block erzeugen.** `cargo xtask test-consensus-manipulation`
   (Phaseâ€‘2-Neuzugang im CLI) baut zunÃ¤chst ein Konsenszertifikat mit gÃ¼ltigem
   Witness, verifiziert den Proof gegen den aktiven Backend-Verifier und nutzt
   erst danach Mutationen. Aktiviere die gewÃ¼nschten Backends mit
   `--features backend-plonky3 --no-default-features` bzw.
   `XTASK_NO_DEFAULT_FEATURES=1 XTASK_FEATURES="prod,prover-stwo"`. Der Lauf
   muss die "baseline consensus proof should verify"-Assertions erreichen â€“ sie
   bestÃ¤tigen, dass der Drill mit einem bekannten guten Block startet, bevor
   Manipulationen injiziert werden.ã€F:xtask/src/main.rsâ€ L144-L190ã€‘ã€F:tests/consensus/consensus_certificate_tampering.rsâ€ L110-L198ã€‘

2. **Manipulierten Replay auslÃ¶sen.** Im Anschluss permutiert der Test
   automatisch VRF-EintrÃ¤ge sowie die Quorum-Roots und erwartet Verifier-Fehler.
   Alternativ lÃ¤sst sich das Phaseâ€‘2-Simnet-Szenario `consensus_quorum_stress`
   per `cargo run -p simnet -- --scenario ... --artifacts-dir ...` starten, um
   valide und manipulierte BlÃ¶cke unter Produktionslast gegeneinander antreten
   zu lassen.ã€F:tests/consensus/consensus_certificate_tampering.rsâ€ L128-L222ã€‘ã€F:tools/simnet/scenarios/consensus_quorum_stress.ronâ€ L1-L22ã€‘

3. **Logs und RPCs auswerten.** Tamper-Erfolge mÃ¼ssen mit Fehlern wie
   `consensus witness participants do not match commit set` und
   `local consensus proof rejected by verifier` im Log enden; die Simnet-LÃ¤ufe
   schreiben sie nach `target/simnet/consensus-quorum/node.log`.
   ErgÃ¤nzend zeigt `GET /status/consensus`, ob der Drill einen gÃ¼ltigen Block
   (`quorum_reached=true`, monotone `round`) oder eine Ablehnung
   (`quorum_reached=false`, Fehlergrund in den Logs) produziert hat.ã€F:rpp/runtime/types/block.rsâ€ L2280-L2314ã€‘ã€F:rpp/runtime/node.rsâ€ L6323-L6466ã€‘ã€F:rpp/rpc/api.rsâ€ L2336-L2344ã€‘

4. **Metriken und Nachweise sichern.** Exportiere Prometheus/Grafana-SchnappschÃ¼sse
   fÃ¼r `consensus_vrf_verification_time_ms` und
   `consensus_quorum_verifications_total`, um Phaseâ€‘2-Limits zu belegen. Das
   Observability-Runbook fÃ¼hrt die erforderlichen Artefakte auf.ã€F:rpp/runtime/telemetry/metrics.rsâ€ L60-L339ã€‘ã€F:docs/dashboards/consensus_grafana.jsonâ€ L1-L200ã€‘ã€F:docs/runbooks/observability.mdâ€ L27-L69ã€‘

5. **Freigabe-Checkliste abhaken.** ErgÃ¤nze die Ergebnisse im
   [Phaseâ€‘2 Acceptance Checklist](./runbooks/phase2_acceptance.md), damit
   Auditor:innen vor Release-Promotion prÃ¼fen kÃ¶nnen, ob alle Guardrails greifen.
   Die Checkliste erwartet verlinkte Logs, RPC-Snapshots und Dashboard-Beweise
   fÃ¼r jede ManipulationsprÃ¼fung.ã€F:docs/runbooks/phase2_acceptance.mdâ€ L1-L39ã€‘

> ğŸ’¡ ErgÃ¤nze jeden Testlauf in der [Observability-Checkliste](./runbooks/observability.md#phase-2-consensus-proof-audits)
> und verlinke die Log-/Dashboard-Screenshots, damit Auditor:innen die Belege
> schnell nachvollziehen kÃ¶nnen.

Common tasks include:

```sh
# Rotate VRF keys using the configured secrets backend
cargo run -p rpp-chain -- validator vrf rotate --config config/validator.toml

# Inspect collector health by querying the validator telemetry endpoint
cargo run -p rpp-chain -- validator telemetry --rpc-url http://127.0.0.1:7070 --auth-token $RPP_RPC_TOKEN --pretty

# Submit and inspect uptime proofs via the validator RPC
cargo run -p rpp-chain -- validator uptime submit --wallet-config config/wallet.toml --auth-token $RPP_RPC_TOKEN
cargo run -p rpp-chain -- validator uptime status --rpc-url http://127.0.0.1:7070 --auth-token $RPP_RPC_TOKEN --json
```

**OTLP failover:** Set `rollout.telemetry.failover_enabled = true` and populate
`secondary_endpoint`/`secondary_http_endpoint` to keep exporters online when the
primary collector is misconfigured. The runtime logs `failed over to secondary`
per sink and increments `telemetry_otlp_failures_total{phase="init_failover"}`
so dashboards can distinguish successful failovers from hard failures. TLS
material is validated before attempting the primary endpoint; malformed
certificates trigger the failover path while keeping the node running.ã€F:rpp/runtime/config.rsâ€ L3633-L3699ã€‘ã€F:rpp/node/src/lib.rsâ€ L1644-L1725ã€‘ã€F:tests/observability_otlp_failures.rsâ€ L109-L212ã€‘

Verwende fÃ¼r `/state-sync`-Operationen die Snapshot-Subcommands statt ad-hoc
`curl`-Aufrufen. `cargo run -p rpp-chain -- validator snapshot status --session <id>` spiegelt die
Light-Client-SSE-Header, sodass Du den Ablauf direkt in der CLI nachvollziehen
kannst. Das Runbook [`network_snapshot_failover`](./runbooks/network_snapshot_failover.md)
fÃ¼hrt Peer-Rotation und Failover-Schritte aus, wÃ¤hrend die
[Phaseâ€‘3-Checkliste](./runbooks/phase3_acceptance.md#snapshot-slis--replay-evidenz)
die notwendigen Artefakte sammelt.ã€F:rpp/node/src/main.rsâ€ L118-L310ã€‘ã€F:docs/runbooks/network_snapshot_failover.mdâ€ L1-L176ã€‘

## RPC authentication & rate limiting

Node RPC endpoints support optional bearer-token authentication and per-client
rate limiting. Supply the configured token with the `--auth-token` flag when
using CLI helpers or add an `Authorization: Bearer <token>` header to curl
requests.ã€F:docs/API_SECURITY.mdâ€ L10-L37ã€‘ã€F:rpp/node/src/main.rsâ€ L101-L178ã€‘ The
flag must match the token configured for the active RPC endpoint; omit it only
when authentication is disabled. Secure configurations should rotate tokens
alongside other secrets and audit usage via reverse-proxy logs.

Expose the RPC to browser dashboards by setting `network.rpc.allowed_origin` in
configuration. Use `--rpc-allowed-origin` for one-off overrides (pass an empty
string to clear the allow-list) and restart without the flag to fall back to the
profile defaults.ã€F:docs/API_SECURITY.mdâ€ L38-L58ã€‘

When automation calls the REST endpoints directly, reuse the same tokens and
respect the configured request limits. A `429` response indicates that the node's
rate limiter rejected the request; retry with exponential backoff or throttle
callers as described in the [deployment & observability playbook](./deployment_observability.md).ã€F:docs/deployment_observability.mdâ€ L1-L61ã€‘

## Example RPC workflows

Automation typically interacts with the node via authenticated HTTP requests.
The pruning runbook outlines the snapshot APIs, including example `curl`
invocations that enqueue pruning work and inspect receipts.ã€F:docs/runbooks/pruning.mdâ€ L1-L102ã€‘
Additional operational runbooks cover startup validation, telemetry wiring, and
upgrade procedures when rolling new binaries into service.ã€F:docs/runbooks/startup.mdâ€ L1-L40ã€‘ã€F:docs/runbooks/observability.mdâ€ L1-L120ã€‘ã€F:docs/runbooks/upgrade.mdâ€ L1-L60ã€‘

Pair this guide with the validator quickstart and troubleshooting references to
fully provision and maintain a production node.ã€F:docs/validator_quickstart.mdâ€ L1-L238ã€‘ã€F:docs/validator_troubleshooting.mdâ€ L1-L140ã€‘
