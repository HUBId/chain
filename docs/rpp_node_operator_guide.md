# `rpp-node` Operator Guide

This guide documents the CLI tooling that ships with the repository. The
`rpp-node` binary hosts the runtime launchers for every supported mode and the
validator maintenance subcommands. No standalone `rpc-cli` tool exists in this
workspaceâ€”the shipped operator interface is the `rpp-node` CLI and the REST/RPC
workflows exposed by the running node.

> **PhaseÂ 2 update:** The `backend-plonky3` feature now enables the vendor
> Plonky3 prover/verifier pipeline. Production builds may target either the
> STWO (`prover-stwo`/`prover-stwo-simd`) or Plonky3 backend; only the
> deterministic mock backend remains blocked in release artefacts. Use the
> release pipeline checklist to verify that binaries are compiled with one of
> the production backends and that mock features are absent from build metadata.
> [`feature_guard.rs`](../rpp/node/src/feature_guard.rs) Â·
> [`build_release.sh`](../scripts/build_release.sh) Â·
> [`verify_release_features.sh`](../scripts/verify_release_features.sh) Â·
> [`ensure_prover_backend`](../rpp/node/src/lib.rs) Â·
> [Release pipeline checklist](../RELEASE.md#release-pipeline-checklist)
> Â· [Plonky3 runbook](./runbooks/plonky3.md)

## Build and install the CLI

Compile the binary with the release profile and select the backend that matches
the deployment tier. The build installs the multiplexer binary at
`target/release/rpp-node` and enables validator functionality required in
staging and production deployments.ã€F:docs/validator_quickstart.mdâ€ L24-L56ã€‘

```sh
# STWO backend
cargo build --release -p rpp-node --no-default-features --features prod,prover-stwo

# Plonky3 backend
cargo build --release -p rpp-node --no-default-features --features prod,backend-plonky3
```

The automated release pipeline exports `RPP_RELEASE_BASE_FEATURES` before
invoking `scripts/build_release.sh`. Point the variable to
`"prod,prover-stwo"`, `"prod,prover-stwo-simd"`, or `"prod,backend-plonky3"`
depending on the backend you intend to ship; the script always forces
`--no-default-features --features "$RPP_RELEASE_BASE_FEATURES"` so every
published artifact includes a production prover and the mock backend remains
disabled.ã€F:.github/workflows/release.ymlâ€ L115-L158ã€‘ã€F:scripts/build_release.shâ€ L1-L118ã€‘
Local builds should mirror the same flag set shown in
`scripts/build_release.sh` to avoid shipping binaries that fail at runtime due
to a missing production backend.

The helper `scripts/verify_release_features.sh` checks the compiled metadata and
fails when the mock prover slips into the feature list; run it as part of
pre-release validation. The compile-time guard mirrors this behaviour by
preventing `backend-plonky3` and `prover-mock` from being enabled at the same
time.ã€F:scripts/verify_release_features.shâ€ L1-L146ã€‘ã€F:rpp/node/src/feature_guard.rsâ€ L1-L5ã€‘

Keep the repository cloned on the host to rebuild quickly when upgrades ship.

## Runtime launchers

The `rpp-node` binary dispatches to four runtime modes and accepts the shared
runtime options (configuration paths, telemetry overrides, dry runs, log
formatting, and networking flags).ã€F:rpp/node/src/main.rsâ€ L28-L75ã€‘ã€F:rpp/node/src/lib.rsâ€ L35-L314ã€‘

```text
rpp-node node [runtime options]
rpp-node wallet [runtime options]
rpp-node hybrid [runtime options]
rpp-node validator [runtime options] [validator subcommand]
```

Pass `--config`/`--wallet-config` to target custom configuration files or rely
on the precedence chain described in the validator quickstart when the defaults
are sufficient.ã€F:docs/validator_quickstart.mdâ€ L62-L111ã€‘ Add `--dry-run` to
validate configuration without starting long-running tasks; the CLI exits after
bootstrap so operators can gate deployments in CI.ã€F:docs/validator_quickstart.mdâ€ L195-L210ã€‘

### Plonky3 backend (Phase 2)

`backend-plonky3` aktiviert jetzt den produktiven Plonky3-Prover und -Verifier.
Die Wallet- und Node-Adapter erzeugen und prÃ¼fen Vendor-Beweise Ã¼ber dieselben
Traits wie das STWO-Backend, sodass Keygen-, Prover- und Verifier-Hooks im
Produktionspfad identisch orchestriert werden.ã€F:rpp/proofs/plonky3/prover/mod.rsâ€ L19-L520ã€‘ã€F:rpp/proofs/plonky3/verifier/mod.rsâ€ L1-L212ã€‘

Das [Plonky3-Runbook](./runbooks/plonky3.md) beschreibt den vollstÃ¤ndigen
Operator-Workflow:

- Circuit-Caches vorbereiten (`rpp-node` legt Proving-/Verifying-Keys im
  Artefaktverzeichnis ab und spiegelt den Zustand Ã¼ber
  `backend_health.plonky3.*`).
- Proof-Generierung und -Verifikation Ã¼berwachen (`rpp.runtime.proof.generation.*`
  und `rpp.runtime.proof.verification.*` mit Labels `backend="plonky3"`,
  `proof="transaction|state|pruning|consensus"`).
- Acceptance-Kriterien prÃ¼fen: die Phaseâ€‘2-Grenzwerte orientieren sich an den
  Messwerten aus `tools/simnet/scenarios/consensus_quorum_stress.ron` und sind in
  `performance/consensus_proofs.md` dokumentiert.

Das Nightly-Szenario `consensus-quorum-stress` treibt den Prover mit hoher
Validator- und Witness-Last sowie absichtlich manipulierten VRF-/Quorum-Daten
an. `scripts/analyze_simnet.py` wertet die JSON-Summary aus, bricht bei
Ãœberschreitung der p95-Grenzen ab und meldet unerwartete Tamper-Erfolge. Die
GegenÃ¼berstellung von Erfolgs- und Fehlerpfaden ist ebenfalls im Runbook
festgehalten.ã€F:tools/simnet/scenarios/consensus_quorum_stress.ronâ€ L1-L22ã€‘ã€F:scripts/analyze_simnet.pyâ€ L1-L200ã€‘ã€F:docs/performance/consensus_proofs.mdâ€ L1-L160ã€‘

Die Konsensus-Beweise werden zusÃ¤tzlich softwareseitig gehÃ¤rtet: `ConsensusCircuit`
im Backend bindet VRF-Ausgaben, -Beweise und Quorum-Digests an den Block-Hash,
die Wallet-Adapter lassen nur valide Zeugen in die Prover-Pipeline, und der
Verifier rekonstruiert die Bindings vor der Proof-PrÃ¼fung. Regressionstests in
`tests/consensus/plonky3_consensus.rs` sowie im Backend sichern diese Checks gegen
Regressionen.ã€F:prover/plonky3_backend/src/circuits/consensus.rsâ€ L1-L245ã€‘ã€F:rpp/proofs/plonky3/prover/mod.rsâ€ L123-L520ã€‘ã€F:rpp/proofs/plonky3/verifier/mod.rsâ€ L1-L212ã€‘ã€F:tests/consensus/plonky3_consensus.rsâ€ L1-L134ã€‘

Grafana-Panels unter `docs/dashboards/consensus_proof_validation.json`
visualisieren diese Kennzahlen (Latenzen, Fehlerraten, Circuit-Cache-GrÃ¶ÃŸe) fÃ¼r
Plonky3 und werden vom CI-Dashboard-Lint Ã¼berprÃ¼ft. Binde die Panels in das
Produktions-Dashboard ein, um Phaseâ€‘2-Abnahmekriterien sichtbar zu machen.ã€F:docs/dashboards/consensus_proof_validation.jsonâ€ L1-L120ã€‘

Kombiniere `backend-plonky3` nicht mit dem `prover-mock`-Feature; der Guard
erzwingt weiterhin die Trennung zwischen deterministischen Test-Fixtures und
produktiven Vendor-Artefakten.ã€F:rpp/node/src/feature_guard.rsâ€ L1-L5ã€‘

## Validator tooling

Invoke validator-specific helpers through `rpp-node validator`. Subcommands cover
VRF rotation, telemetry diagnostics, and uptime proof management, all backed by
the active node configuration.ã€F:rpp/node/src/main.rsâ€ L48-L183ã€‘ Detailed
workflowsâ€”including sample invocations and expected outputâ€”live in the
[validator tooling guide](./validator_tooling.md).ã€F:docs/validator_tooling.mdâ€ L14-L137ã€‘

### Consensus proof metadata expectations

Finality proofs now encode the epoch/slot context, VRF proofs, and quorum
evidence roots inside the public inputs. Operators must ensure that
`consensus.metadata` in block production includes:

- `epoch`/`slot` counters that match the fork-choice state machine.
- Hex-encoded `quorum_bitmap_root` and `quorum_signature_root` digests from the
  aggregated vote sets.
- VollstÃ¤ndige `vrf_entries` inklusive Randomness, Pre-Output, Proof, Public
  Key sowie Poseidon-Metadaten (`Digest`, `Last Block Header`, `Epoch`,
  `Tier Seed`) fÃ¼r jede Validator:in im Zertifikat. `Last Block Header` muss dem
  Zertifikats-`block_hash` entsprechen und die `Epoch`-Zeichenkette dem
  exportierten `epoch`-ZÃ¤hler. Ã„ltere Clients kÃ¶nnen die bisherigen
  `vrf_outputs`/`vrf_proofs` aus diesen EintrÃ¤gen ableiten, solange sie das
  Version-Flag `version=2` setzen.

Setze bei RPC-Checks nach MÃ¶glichkeit `version=3`, um die strukturierten
EintrÃ¤ge inklusive Public Keys und Poseidon-Digests zu erhalten. TemporÃ¤re
KompatibilitÃ¤ts-Pipelines dÃ¼rfen weiterhin `version=2` anfordern, sollten aber
das Downstream-Mapping aus `vrf_entries` dokumentieren, um den Wechsel
nachvollziehbar zu halten.

Missing or inconsistent values cause the verifier to reject the consensus proof
bundle, so double-check the witness payload when diagnosing failed block
imports.ã€F:docs/consensus/finality_proof_story.mdâ€ L1-L38ã€‘

Release-Builds listen Circuit-Versionen, Constraint-ZÃ¤hlungen und unterstÃ¼tzte Backends
in den [Release-Notizen](docs/release_notes.md); ziehe die Tabelle bei Audits oder
Rollback-PlÃ¤nen hinzu, um sicherzustellen, dass Operator:innen identische Proof-Artefakte
ausrollen.ã€F:docs/release_notes.mdâ€ L1-L160ã€‘

### Phaseâ€¯2 consensus proof validation checks

Phaseâ€¯2 verlangt nachvollziehbare Belege, dass manipulierte VRF-/Quorum-Daten an
der Validator-Schnittstelle scheitern.

#### Known-good vs. tampered replay drill

Nutze den Phaseâ€‘2-Workflow, um sowohl einen gÃ¼ltigen Block als auch abgelehnte
Manipulationen zu dokumentieren:

1. **Bekannten guten Block erzeugen.** `cargo xtask test-consensus-manipulation`
   (Phaseâ€‘2-Neuzugang im CLI) baut zunÃ¤chst ein Konsenszertifikat mit gÃ¼ltigem
   Witness, verifiziert den Proof gegen den aktiven Backend-Verifier und nutzt
   erst danach Mutationen. Aktiviere die gewÃ¼nschten Backends mit
   `--features backend-plonky3 --no-default-features` bzw.
   `XTASK_NO_DEFAULT_FEATURES=1 XTASK_FEATURES="prod,prover-stwo"`. Der Lauf
   muss die "baseline consensus proof should verify"-Assertions erreichen â€“ sie
   bestÃ¤tigen, dass der Drill mit einem bekannten guten Block startet, bevor
   Manipulationen injiziert werden.ã€F:xtask/src/main.rsâ€ L144-L190ã€‘ã€F:tests/consensus/consensus_certificate_tampering.rsâ€ L110-L198ã€‘

2. **Manipulierten Replay auslÃ¶sen.** Im Anschluss permutiert der Test
   automatisch VRF-EintrÃ¤ge sowie die Quorum-Roots und erwartet Verifier-Fehler.
   Alternativ lÃ¤sst sich das Phaseâ€‘2-Simnet-Szenario `consensus_quorum_stress`
   per `cargo run -p simnet -- --scenario ... --artifacts-dir ...` starten, um
   valide und manipulierte BlÃ¶cke unter Produktionslast gegeneinander antreten
   zu lassen.ã€F:tests/consensus/consensus_certificate_tampering.rsâ€ L128-L222ã€‘ã€F:tools/simnet/scenarios/consensus_quorum_stress.ronâ€ L1-L22ã€‘

3. **Logs und RPCs auswerten.** Tamper-Erfolge mÃ¼ssen mit Fehlern wie
   `consensus witness participants do not match commit set` und
   `local consensus proof rejected by verifier` im Log enden; die Simnet-LÃ¤ufe
   schreiben sie nach `target/simnet/consensus-quorum/node.log`.
   ErgÃ¤nzend zeigt `GET /status/consensus`, ob der Drill einen gÃ¼ltigen Block
   (`quorum_reached=true`, monotone `round`) oder eine Ablehnung
   (`quorum_reached=false`, Fehlergrund in den Logs) produziert hat.ã€F:rpp/runtime/types/block.rsâ€ L2280-L2314ã€‘ã€F:rpp/runtime/node.rsâ€ L6323-L6466ã€‘ã€F:rpp/rpc/api.rsâ€ L2336-L2344ã€‘

4. **Metriken und Nachweise sichern.** Exportiere Prometheus/Grafana-SchnappschÃ¼sse
   fÃ¼r `consensus_vrf_verification_time_ms` und
   `consensus_quorum_verifications_total`, um Phaseâ€‘2-Limits zu belegen. Das
   Observability-Runbook fÃ¼hrt die erforderlichen Artefakte auf.ã€F:rpp/runtime/telemetry/metrics.rsâ€ L60-L339ã€‘ã€F:docs/dashboards/consensus_grafana.jsonâ€ L1-L200ã€‘ã€F:docs/runbooks/observability.mdâ€ L27-L69ã€‘

5. **Freigabe-Checkliste abhaken.** ErgÃ¤nze die Ergebnisse im
   [Phaseâ€‘2 Acceptance Checklist](./runbooks/phase2_acceptance.md), damit
   Auditor:innen vor Release-Promotion prÃ¼fen kÃ¶nnen, ob alle Guardrails greifen.
   Die Checkliste erwartet verlinkte Logs, RPC-Snapshots und Dashboard-Beweise
   fÃ¼r jede ManipulationsprÃ¼fung.ã€F:docs/runbooks/phase2_acceptance.mdâ€ L1-L39ã€‘

> ğŸ’¡ ErgÃ¤nze jeden Testlauf in der [Observability-Checkliste](./runbooks/observability.md#phase-2-consensus-proof-audits)
> und verlinke die Log-/Dashboard-Screenshots, damit Auditor:innen die Belege
> schnell nachvollziehen kÃ¶nnen.

Common tasks include:

```sh
# Rotate VRF keys using the configured secrets backend
rpp-node validator vrf rotate --config config/validator.toml

# Inspect collector health by querying the validator telemetry endpoint
rpp-node validator telemetry --rpc-url http://127.0.0.1:7070 --auth-token $RPP_RPC_TOKEN --pretty

# Submit and inspect uptime proofs via the validator RPC
rpp-node validator uptime submit --wallet-config config/wallet.toml --auth-token $RPP_RPC_TOKEN
rpp-node validator uptime status --rpc-url http://127.0.0.1:7070 --auth-token $RPP_RPC_TOKEN --json
```

State-sync and head monitoring rely on the public `/state-sync` RPC endpoints;
use `curl`/`wget` or similar tooling to consume the SSE stream and fetch
snapshot chunks as outlined in the validator tooling guide.ã€F:docs/validator_tooling.mdâ€ L53-L118ã€‘

## RPC authentication & rate limiting

Node RPC endpoints support optional bearer-token authentication and per-client
rate limiting. Supply the configured token with the `--auth-token` flag when
using CLI helpers or add an `Authorization: Bearer <token>` header to curl
requests.ã€F:docs/API_SECURITY.mdâ€ L10-L37ã€‘ã€F:rpp/node/src/main.rsâ€ L101-L178ã€‘ The
flag must match the token configured for the active RPC endpoint; omit it only
when authentication is disabled. Secure configurations should rotate tokens
alongside other secrets and audit usage via reverse-proxy logs.

Expose the RPC to browser dashboards by setting `network.rpc.allowed_origin` in
configuration. Use `--rpc-allowed-origin` for one-off overrides (pass an empty
string to clear the allow-list) and restart without the flag to fall back to the
profile defaults.ã€F:docs/API_SECURITY.mdâ€ L38-L58ã€‘

When automation calls the REST endpoints directly, reuse the same tokens and
respect the configured request limits. A `429` response indicates that the node's
rate limiter rejected the request; retry with exponential backoff or throttle
callers as described in the [deployment & observability playbook](./deployment_observability.md).ã€F:docs/deployment_observability.mdâ€ L1-L61ã€‘

## Example RPC workflows

Automation typically interacts with the node via authenticated HTTP requests.
The pruning runbook outlines the snapshot APIs, including example `curl`
invocations that enqueue pruning work and inspect receipts.ã€F:docs/runbooks/pruning.mdâ€ L1-L102ã€‘
Additional operational runbooks cover startup validation, telemetry wiring, and
upgrade procedures when rolling new binaries into service.ã€F:docs/runbooks/startup.mdâ€ L1-L40ã€‘ã€F:docs/runbooks/observability.mdâ€ L1-L120ã€‘ã€F:docs/runbooks/upgrade.mdâ€ L1-L60ã€‘

Pair this guide with the validator quickstart and troubleshooting references to
fully provision and maintain a production node.ã€F:docs/validator_quickstart.mdâ€ L1-L238ã€‘ã€F:docs/validator_troubleshooting.mdâ€ L1-L140ã€‘
