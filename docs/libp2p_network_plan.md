# Libp2p-Netzwerk-Backbone â€“ Umsetzungsplan

Dieser Plan gliedert die Umsetzung des Blueprintâ€¯2.3 in klar umrissene LiefergegenstÃ¤nde mit definierten AbhÃ¤ngigkeiten, QualitÃ¤tskriterien und Tests. Jeder Abschnitt endet mit â€žDefinition of Doneâ€œ (DoD), damit Fortschritt messbar bleibt.

> **Aktueller Stand:** Die Ã¶ffentliche RPC-Schnittstelle verwaltet keinen Libp2p-Knoten. Runtime-Handles werden ausschlieÃŸlich innerhalb der Runtime verwendet und nicht via `ApiContext` exponiert. Operator-Steuerung des Netzwerks bleibt damit ausdrÃ¼cklich Teil der nachstehenden Deliverables.

## Phase 0 â€“ Grundlagen & Infrastruktur
1. **Libp2p-AbhÃ¤ngigkeiten und Runtime integrieren**
   - `libp2p`-, `tokio`- und `futures`-Versionen wÃ¤hlen, die mit dem bestehenden Async-Stack kompatibel sind.
   - Eigenes `NodeKeypair` (Ed25519) auf Persistenz heben.
   - DoD: `cargo check`, `cargo fmt`, Smoke-Test mit initiiertem Swarm.

2. **Noise-XX Handshake + Peerstore**
   - Noise-Konfiguration mit statischem SchlÃ¼ssel + Ephemeral Keypair aufsetzen.
   - ZSI-ID + VRF-Proof im Handshake transportieren (Custom Protocol oder secio Peer Records).
   - Peerstore implementieren (Memory + persistente Cache-Option) mit Reputation-/Tier-Attributen.
   - DoD: Zwei Nodes stellen Noise-XX Verbindung her und tauschen Metadaten aus.

3. **Kanal- und Topic-Definitionen**
   - GossipSub-Topics (`blocks`, `votes`, `proofs`, `snapshots`, `meta`) als Konstanten inkl. Message-Schema definieren.
   - Canonical Binary Encoding (z.â€¯B. Prost) festlegen, Schema-Validierungen via unit tests.
   - DoD: Message-Typen und Encoding-Tests vorhanden, Topics registriert.

## Phase 1 â€“ Gossip-Backbone & Admission Control
4. **GossipSub-Konfiguration**
   - Mesh/Peer-Score-Parameter an Reputation/Tier koppeln.
   - Heartbeat, Mesh-Pruning und Score-Decay konfigurieren.
   - DoD: Nodes kÃ¶nnen sich gegenseitig Nachrichten in allen Topics senden, Peer-Score aktualisiert sich.

5. **Admission Control Layer**
   - Reputation-/Tier-Logik anbinden: TL0 (read-only), TL1 (Proof-Publish), TL3+ (Consensus).
   - Subscription-Gating pro Topic, Reject-Mechanismen bei Downgrades oder Sperrlisten.
   - DoD: Integrationstest simuliert Peers verschiedener Tiers und Ã¼berprÃ¼ft Zugriffe.

6. **Reputation-Updates & Sperrlisten**
   - Netzwerkweite Reputation-Events (Uptime, Slashing, Votes) konsumieren und Peerstore aktualisieren.
   - Dynamic Penalties (Score-Decay, Ban Windows) implementieren.
   - DoD: Telemetrie zeigt Reputation-Ã„nderungen, gebannte Peers verlieren Zugriff.

## Phase 2 â€“ Datenpfade & Synchronisation
7. **Proofs- und Transaction-Pipeline**
   - Gossip-Nachrichten in lokale Queues (Proof-Mempool) einspeisen.
   - Deduplication + Validation Hooks (STWO, Reputation) einbauen.
   - DoD: Eingehende Proofs werden persistiert und validiert.

8. **Blocks & Votes**
   - BlockvorschlÃ¤ge Ã¼ber `blocks` Topic verteilen, Votes Ã¼ber `votes` Topic.
   - Konsensmodul an Gossip-Ereignisse anschlieÃŸen (Proposal, PreVote, PreCommit).
   - DoD: Multi-Node-Test produziert Block mit >=2/3 Votes.

9. **Snapshots & Light-Client-Sync**
   - Streaming von Firewood-Snapshots via libp2p-Request/Response oder Chunked Gossip.
   - Light-Clients empfangen nur Headers + Recursive Proofs, validieren Roots.
   - DoD: Light-Client-Test lÃ¤dt Snapshot + Proof, validiert erfolgreich.

10. **Meta-Kanal & Telemetrie**
    - Peer-Heartbeat, Latenzen, Versionen Ã¼ber `meta` Topic publizieren.
    - Dashboard/Prometheus-Exporter fÃ¼ttern.
    - DoD: Telemetrie-Events sichtbar, Alerts bei Offline-Peers.

## Phase 3 â€“ Robustheit & Betrieb
11. **Persistence & Crash-Recovery**
    - Peerstore + Gossip-State auf Disk sichern.
    - Rejoin-Logic fÃ¼r Mesh/Topics nach Neustart.
    - DoD: Node rebooted und nimmt Mesh wieder auf ohne manuellen Eingriff.

12. **Security Hardening**
    - Replay-Schutz (Message-IDs, seqno), Flood-Prevention, Rate-Limits.
    - Fuzzing/Property-Tests fÃ¼r Decoder und Admission-Control.
    - Konfigurierbare Zugriffslisten: `p2p.allowlist` (Default: `[]`) fixiert Tier-Level erlaubter Peers,
      `p2p.blocklist` (Default: `[]`) sperrt Peers dauerhaft bereits beim Handshake.
    - DoD: Sicherheits-Tests laufen in CI, bekannte Angriffsvektoren mitigiert.

13. **Simulation & Stresstests**
    - Netz-Simulator (lokal via Tokio oder verteilte Container) mit 20+ Peers.
    - Metriken fÃ¼r Latenz, Gossip-Mesh-StabilitÃ¤t, Reputation-Drift.
    - DoD: Simulation lÃ¤uft automatisiert, produziert Report.

## Cross-Cutting Deliverables
- **Dokumentation:** Architekturgrafik, Config-Referenz, Operator-Guides.
- **CI-Integration:** Linting, Unit-Tests, Integrationstests, Simulation Entry-Points.
- **Observability:** Logs (structured), Tracing spans, Metrics.

## AbhÃ¤ngigkeiten & Milestones
1. *Milestoneâ€¯A (Phasen 0â€“1):* FunktionsfÃ¤higer Gossip-Backbone mit Admission-Control. **Status:** âœ… Tier-basierte Zugriffslogik aktiv, abgesichert durch Integrationstests (`rpp/p2p/tests/access_control.rs`).
2. *Milestoneâ€¯B (Phaseâ€¯2):* Block- und Snapshot-Datenpfade live, Light-Client-Sync mÃ¶glich. **Status:** ðŸš§ Datenpfade folgen nach Abschluss der Gossip-Gating-Tests (`rpp/p2p/tests/access_control.rs`).
3. *Milestoneâ€¯C (Phaseâ€¯3):* Produktionsreife mit Security, Persistenz, Simulation.

Jede Milestone-Abnahme setzt neben den DoD-Kriterien auch Peer-to-Peer-Tests Ã¼ber mindestens drei Knoten voraus.
